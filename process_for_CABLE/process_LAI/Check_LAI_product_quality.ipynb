{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3aa4379-6d96-4d43-97b7-4f84dd0b85c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import glob\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import netCDF4 as nc\n",
    "import multiprocessing as mp\n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib.cm import get_cmap\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib import colors\n",
    "import cartopy.crs as ccrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81e23061-c2ac-49f3-9146-1c3094763850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use PLUMBER2_GPP_common_utils, change directory to where it exists\n",
    "os.chdir('/g/data/w97/mm3972/scripts/Land_Drought_Rainfall')\n",
    "from common_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659c4d12-bceb-4b31-b675-345b80432a52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Compare PLUMBER2 site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f1420-00e2-4050-828e-848d8772a4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PLUMBER2_lat_lon_LAI(PLUMBER2_AU_file, au_site_list, start_date, end_date):\n",
    "\n",
    "    site_name  = PLUMBER2_AU_file.split('_')[0] \n",
    "    print(site_name)\n",
    "\n",
    "    # Read PLUMBER2 file\n",
    "    f_PLUMBER2 = nc.Dataset(f'{PLUMBER2_path}/{PLUMBER2_AU_file}') \n",
    "\n",
    "    df_P2      = pd.DataFrame(f_PLUMBER2.variables['LAI'][:,0,0].data, columns=['LAI'])\n",
    "\n",
    "    # Read PLUMBER2 time\n",
    "    P2_time    = nc.num2date(f_PLUMBER2.variables['time'][:], \n",
    "                             f_PLUMBER2.variables['time'].units, \n",
    "                             only_use_cftime_datetimes=False,\n",
    "                             only_use_python_datetimes=True)\n",
    "        \n",
    "    ntime = len(P2_time)\n",
    "    year  = np.zeros(ntime)\n",
    "    month = np.zeros(ntime)\n",
    "    day   = np.zeros(ntime)\n",
    "    \n",
    "    # Extract year, month, day\n",
    "    for tt, t in enumerate(P2_time):\n",
    "        year[tt]  = t.year\n",
    "        month[tt] = t.month\n",
    "        day[tt]   = t.day\n",
    "\n",
    "    # Make time array for processed LAI  \n",
    "    start_date_P2  = datetime(int(year[0]),  int(month[0]),  int(day[0]), 0, 0)\n",
    "    end_date_P2    = datetime(int(year[-1]), int(month[-1]), int(day[-1]), 0, 0)\n",
    "\n",
    "    # Create an array of datetime objects\n",
    "    P2_time_new    = np.array([start_date_P2 + timedelta(days=i) for i in range((end_date_P2 - start_date_P2).days + 1)])\n",
    "    \n",
    "    time_mask      = (P2_time_new>=start_date) & (P2_time_new<=end_date)\n",
    "    \n",
    "    if np.sum(time_mask)>10:\n",
    "\n",
    "        df_P2['year']  = year\n",
    "        df_P2['month'] = month\n",
    "        df_P2['day']   = day\n",
    "        df_P2          = df_P2.groupby(['year', 'month', 'day']).mean(numeric_only=True).reset_index()\n",
    "\n",
    "        # Get lat and lon for the site\n",
    "        lat = au_site_characters.loc[au_site_characters['site_name'] == site_name, 'lat'].values[0]\n",
    "        lon = au_site_characters.loc[au_site_characters['site_name'] == site_name, 'lon'].values[0]\n",
    "        # print(lat, lon)\n",
    "\n",
    "        df_P2_tmp         = df_P2.loc[time_mask]\n",
    "        df_P2_tmp['time'] = P2_time_new[time_mask]\n",
    "\n",
    "        return site_name, lat, lon, df_P2_tmp\n",
    "    else:\n",
    "        return site_name, np.nan, np.nan, np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13e2ffc-e2fe-4451-9a1c-1858b55dc88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in np.arange(2000,2020):\n",
    "    file_LAI_yuan      = f'/g/data/w97/mm3972/data/MODIS/MODIS_LAI/Sun_Yatsen_Uni_processed_MODIS_LAI_61/lai_8-day_0.05_{year}.nc'\n",
    "    # file_LAI_processed = f'/g/data/w97/mm3972/scripts/Land_Drought_Rainfall/process_for_CABLE/nc_files/gridinfo_AWAP_OpenLandMap_ELEV_DLCM_fix_MODIS_LAI_albedo_lc_time_varying_{year}.nc'\n",
    "    file_LAI_processed = f'/g/data/w97/mm3972/data/MODIS/MODIS_LAI/AUS/regrid_2_AWAP_5km_daily/remove_high_frequent_varibility_method2_smooth_anomaly/MCD15A3H.061_500m_aid0001_LAI_regridded_daily_2000-2023_remove_high_freq.nc'\n",
    "\n",
    "    f_LAI_yuan         = nc.Dataset(file_LAI_yuan)\n",
    "    f_LAI_processed    = nc.Dataset(file_LAI_processed)\n",
    "\n",
    "    LAI_yuan           = f_LAI_yuan.variables['lai'][:]\n",
    "    lat_yuan           = f_LAI_yuan.variables['lat'][:]\n",
    "    lon_yuan           = f_LAI_yuan.variables['lon'][:]\n",
    "    time_yuan          = nc.num2date(f_LAI_yuan.variables['time'][:],f'days since {year}-01-01 00:00:00',\n",
    "                         only_use_cftime_datetimes=False, only_use_python_datetimes=True)\n",
    "\n",
    "    LAI_processed      = f_LAI_processed.variables['LAI'][:]\n",
    "    lat_processed      = f_LAI_processed.variables['latitude'][:]\n",
    "    lon_processed      = f_LAI_processed.variables['longitude'][:]\n",
    "\n",
    "    # Make time array for processed LAI  \n",
    "    start_date         = datetime(year, 1, 1, 0, 0)\n",
    "    end_date           = datetime(year, 12, 31, 0, 0)\n",
    "\n",
    "    # Create an array of datetime objects\n",
    "    # time_processed     = np.array([start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)])\n",
    "    time_processed     = nc.num2date(f_LAI_processed.variables['time'][:],f_LAI_processed.variables['time'].units,\n",
    "                         only_use_cftime_datetimes=False, only_use_python_datetimes=True)\n",
    "\n",
    "    message            = 'Met'\n",
    "    PLUMBER2_path      = '/g/data/w97/mm3972/data/Fluxnet_data/Post-processed_PLUMBER2_outputs/Nc_files/Met'\n",
    "    PLUMBER2_AU_files  = [  f'AU-ASM_2011-2017_OzFlux_{message}.nc',      \n",
    "                            f'AU-Cpr_2011-2017_OzFlux_{message}.nc',     \n",
    "                            f'AU-Cum_2013-2018_OzFlux_{message}.nc',       \n",
    "                            f'AU-DaP_2009-2012_OzFlux_{message}.nc',       \n",
    "                            f'AU-DaS_2010-2017_OzFlux_{message}.nc',       \n",
    "                            f'AU-Dry_2011-2015_OzFlux_{message}.nc',       \n",
    "                            f'AU-Emr_2012-2013_OzFlux_{message}.nc',       \n",
    "                            f'AU-GWW_2013-2017_OzFlux_{message}.nc',       \n",
    "                            f'AU-Gin_2012-2017_OzFlux_{message}.nc',       \n",
    "                            f'AU-How_2003-2017_OzFlux_{message}.nc',       \n",
    "                            f'AU-Lit_2016-2017_OzFlux_{message}.nc',       \n",
    "                            f'AU-Rig_2011-2016_OzFlux_{message}.nc',       \n",
    "                            f'AU-Rob_2014-2017_OzFlux_{message}.nc',       \n",
    "                            f'AU-Sam_2011-2017_OzFlux_{message}.nc',       \n",
    "                            f'AU-Stp_2010-2017_OzFlux_{message}.nc',       \n",
    "                            f'AU-TTE_2013-2017_OzFlux_{message}.nc',       \n",
    "                            f'AU-Tum_2002-2017_OzFlux_{message}.nc',       \n",
    "                            f'AU-Whr_2015-2016_OzFlux_{message}.nc',       \n",
    "                            f'AU-Wrr_2016-2017_OzFlux_{message}.nc',       \n",
    "                            f'AU-Ync_2011-2017_OzFlux_{message}.nc',\n",
    "                            # f'AU-Otw_2009-2010_OzFlux_{message}.nc',     \n",
    "                            # f'AU-Cow_2010-2015_OzFlux_{message}.nc',         \n",
    "                            # f'AU-Ctr_2010-2017_OzFlux_{message}.nc',         \n",
    "                            ]\n",
    "\n",
    "    site_characters    = pd.read_csv('/g/data/w97/mm3972/scripts/PLUMBER2/LSM_GPP_PLUMBER2/txt/site_character.csv')\n",
    "\n",
    "    # Correct filtering for Australian sites\n",
    "    au_site_characters = site_characters[site_characters['site_name'].str.contains('AU')]\n",
    "    au_site_list       = np.unique(au_site_characters['site_name'])\n",
    "\n",
    "    for PLUMBER2_AU_file in PLUMBER2_AU_files:\n",
    "\n",
    "        site_name, lat, lon, df_LAI_P2  = get_PLUMBER2_lat_lon_LAI(PLUMBER2_AU_file, au_site_list, start_date, end_date)\n",
    "        \n",
    "        print(lat, lon, df_LAI_P2)\n",
    "        \n",
    "        if ~np.isnan(lat):\n",
    "            # Get pixel LAI in Yuanhua's file\n",
    "            # Find the indices of the nearest pixels to lat and lon.\n",
    "            lat_idx_yuan      = np.argmin(np.abs(lat_yuan - lat))\n",
    "            lon_idx_yuan      = np.argmin(np.abs(lon_yuan - lon))\n",
    "\n",
    "            # Read the climate_class value of the nearest pixel.\n",
    "            df_LAI_yuan_tmp   = pd.DataFrame(LAI_yuan[:, lat_idx_yuan, lon_idx_yuan].data, columns=['LAI'])\n",
    "\n",
    "            # Subset CABLE data to match PLUMBER2 time range\n",
    "            df_LAI_yuan        = df_LAI_yuan_tmp[(time_yuan >= start_date) & (time_yuan <= end_date)]\n",
    "            df_LAI_yuan['time']= time_yuan[(time_yuan >= start_date) & (time_yuan <= end_date)]\n",
    "\n",
    "            # Get pixel LAI in my file\n",
    "            # Find the indices of the nearest pixels to lat and lon.\n",
    "            lat_idx_processed    = np.argmin(np.abs(lat_processed - lat))\n",
    "            lon_idx_processed    = np.argmin(np.abs(lon_processed - lon))\n",
    "\n",
    "            # Read the climate_class value of the nearest pixel.\n",
    "            df_LAI_processed_tmp = pd.DataFrame(LAI_processed[:, lat_idx_processed, lon_idx_processed].data, columns=['LAI'])\n",
    "\n",
    "            # Subset CABLE data to match PLUMBER2 time range\n",
    "            df_LAI_processed        = df_LAI_processed_tmp[(time_processed >= start_date) & (time_processed <= end_date)]\n",
    "            df_LAI_processed['time']= time_processed[(time_processed >= start_date) & (time_processed <= end_date)]\n",
    "\n",
    "            \n",
    "            # ================== Start Plotting =================\n",
    "            fig, ax  = plt.subplots(figsize=(6,5))\n",
    "\n",
    "            # Plot comparison of Qle for CABLE and PLUMBER2\n",
    "            print(len(df_LAI_yuan['time']), len(df_LAI_processed['time']), len(df_LAI_P2['time']))\n",
    "            \n",
    "            ax.plot(df_LAI_yuan['time'],      df_LAI_yuan['LAI'].values, c='red', label='Yuanhua')\n",
    "            ax.plot(df_LAI_processed['time'], df_LAI_processed['LAI'].values, c='green', label='processed')\n",
    "            ax.plot(df_LAI_P2['time'],        df_LAI_P2['LAI'].values, c='blue', label='PLUMBER2')\n",
    "            ax.legend()\n",
    "            ax.set_title(f'LAI Comparison for {site_name}')\n",
    "\n",
    "            plt.savefig(f'./Check_LAI_{site_name}_{year}.png',dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333a3b92-981f-4135-b16b-4a5cc4b3e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_yuan_mask = lat_yuan >lats[0] & lat_yuan <lats[1]\n",
    "lon_yuan_mask = lon_yuan >lons[0] & lon_yuan <lons[1]\n",
    "LAI_yuan_regmean = np.nanmean(LAI_yuan.loc[:, lat_yuan_mask,lon_yuan_mask],axis=0)\n",
    "\n",
    "lat_processed_mask    = lat_processed >lats[0] & lat_processed <lats[1]\n",
    "lon_processed_mask    = lon_processed >lons[0] & lon_processed <lons[1]\n",
    "LAI_processed_regmean = np.nanmean(LAI_processed.loc[:, lat_processed_mask,lon_processed_mask],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d95a08-c136-4d6f-8b51-a5050c4fa37f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Compare Spatial map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a17b28f-cba8-4f9b-abdc-fb1eabbd109e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_LAI_processed = f'/g/data/w97/mm3972/data/MODIS/MODIS_LAI/AUS/regrid_2_AWAP_5km_daily/remove_high_frequent_varibility_method2_smooth_anomaly/MCD15A3H.061_500m_aid0001_LAI_regridded_daily_2000-2023_remove_high_freq.nc'\n",
    "\n",
    "f_LAI_processed    = nc.Dataset(file_LAI_processed)\n",
    "LAI_processed      = f_LAI_processed.variables['LAI'][:]\n",
    "lat_processed      = f_LAI_processed.variables['latitude'][:]\n",
    "lon_processed      = f_LAI_processed.variables['longitude'][:]\n",
    "time_processed     = nc.num2date(f_LAI_processed.variables['time'][:],f_LAI_processed.variables['time'].units,\n",
    "                     only_use_cftime_datetimes=False, only_use_python_datetimes=True)\n",
    "\n",
    "nlat               = len(lat_processed)\n",
    "nlon               = len(lon_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d93abf-27fa-4552-a5e7-3ba0d77fb011",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in np.arange(2000,2020):\n",
    "    \n",
    "    file_LAI_yuan      = f'/g/data/w97/mm3972/data/MODIS/MODIS_LAI/Sun_Yatsen_Uni_processed_MODIS_LAI_61/lai_8-day_0.05_{year}.nc'\n",
    "\n",
    "    f_LAI_yuan         = nc.Dataset(file_LAI_yuan)\n",
    "    LAI_yuan           = f_LAI_yuan.variables['lai'][:]\n",
    "    lat_yuan           = f_LAI_yuan.variables['lat'][:]\n",
    "    lon_yuan           = f_LAI_yuan.variables['lon'][:]\n",
    "    LAI_yuan_mean      = np.mean(LAI_yuan, axis=0)\n",
    "\n",
    "    # Make time array for processed LAI  \n",
    "    start_date         = datetime(year, 1, 1, 0, 0)\n",
    "    end_date           = datetime(year+1, 1, 1, 0, 0)\n",
    "    LAI_processed_mean = np.mean(LAI_processed[(time_processed>=start_date) & (time_processed<=end_date), :,:], axis=0)\n",
    "\n",
    "    # Regrid\n",
    "    LAI_yuan_mean_regrid= regrid_data(lat_yuan, lon_yuan, lat_processed, lon_processed, \n",
    "                                      LAI_yuan_mean, method='nearest',threshold=None)\n",
    "\n",
    "    # ================== Start Plotting =================\n",
    "    # Create a figure with 3 subplots in 1 row\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(18, 5), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "    # Loop through each axis\n",
    "    for ax in axs:\n",
    "\n",
    "        # Set extent based on loc_lat and loc_lon\n",
    "        ax.set_extent([112, 154, -43, -10])  # Example extent, adjust as needed\n",
    "        ax.coastlines(resolution=\"50m\", linewidth=1)\n",
    "\n",
    "    # Plot windspeed based on variable names\n",
    "    levels1 = [0,0.5,1,1.5,2.,2.5,3,3.5,4.,4.5,5.,5.5,6.,6.5,7.]\n",
    "    levels2 = [-1.,-0.8,-0.6,-0.4,-0.2,0.2,0.4,0.6,0.8,1.]\n",
    "    plot1   = axs[0].contourf(lon_processed, lat_processed, LAI_yuan_mean_regrid, levels=levels1, transform=ccrs.PlateCarree(), extend='both', cmap=plt.cm.BrBG)\n",
    "    plot2   = axs[1].contourf(lon_processed, lat_processed, LAI_processed_mean, levels=levels1, transform=ccrs.PlateCarree(), extend='both', cmap=plt.cm.BrBG)\n",
    "    plot3   = axs[2].contourf(lon_processed, lat_processed, LAI_processed_mean-LAI_yuan_mean_regrid, levels=levels2, transform=ccrs.PlateCarree(), extend='both', cmap=plt.cm.BrBG)\n",
    "\n",
    "    axs[0].set_title('Yuanhua', size=16)\n",
    "    axs[1].set_title('Processed', size=16)\n",
    "    axs[2].set_title('Processed-Yuanhua', size=16)\n",
    "\n",
    "    cb = plt.colorbar(plot1, ax=axs[0], orientation=\"horizontal\", pad=0.02, aspect=16, shrink=0.8)\n",
    "    cb = plt.colorbar(plot2, ax=axs[1], orientation=\"horizontal\", pad=0.02, aspect=16, shrink=0.8)\n",
    "    cb = plt.colorbar(plot3, ax=axs[2], orientation=\"horizontal\", pad=0.02, aspect=16, shrink=0.8)\n",
    "    cb.ax.tick_params(labelsize=10)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./plots/Check_LAI/Check_Yuanhua_my_LAI_against_PLUMBER2_{year}.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e78f745-c610-4986-a267-c5bcb750b609",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check the correlation between LAI and albedo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cec63c-6487-4e10-a205-21432f2fc984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "year               = 2017\n",
    "file_gridinfo_SG   = f'/g/data/w97/mm3972/scripts/Land_Drought_Rainfall/process_for_CABLE/nc_files/gridinfo_AWAP_OpenLandMap_ELEV_DLCM_fix_MODIS_LAI_albedo_lc_time_varying_{year}.nc'\n",
    "file_gridinfo      = f'/g/data/w97/mm3972/scripts/Land_Drought_Rainfall/process_for_CABLE/nc_files/no_SG_31-day_running_smooth/gridinfo_AWAP_OpenLandMap_ELEV_DLCM_fix_MODIS_LAI_albedo_lc_time_varying_{year}.nc'\n",
    "\n",
    "f_SG               = nc.Dataset(file_gridinfo_SG)\n",
    "LAI_SG             = f_SG.variables['LAI'][:]\n",
    "albedo             = f_SG.variables['Albedo_MODIS'][:]\n",
    "lat_lai            = f_SG.variables['latitude'][:]\n",
    "lon_lai            = f_SG.variables['longitude'][:]\n",
    "\n",
    "f                  = nc.Dataset(file_gridinfo)\n",
    "LAI                = f.variables['LAI'][:]\n",
    "\n",
    "# Make time array for processed LAI  \n",
    "start_date         = datetime(year, 1, 1, 0, 0)\n",
    "end_date           = datetime(year, 12, 31, 0, 0)\n",
    "time               = np.array([start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)])\n",
    "    \n",
    "message            = 'Met'\n",
    "PLUMBER2_path      = '/g/data/w97/mm3972/data/Fluxnet_data/Post-processed_PLUMBER2_outputs/Nc_files/Met'\n",
    "PLUMBER2_AU_files  = [  f'AU-ASM_2011-2017_OzFlux_{message}.nc',      \n",
    "                        f'AU-Cpr_2011-2017_OzFlux_{message}.nc',     \n",
    "                        f'AU-Cum_2013-2018_OzFlux_{message}.nc',       \n",
    "                        f'AU-DaP_2009-2012_OzFlux_{message}.nc',       \n",
    "                        f'AU-DaS_2010-2017_OzFlux_{message}.nc',       \n",
    "                        f'AU-Dry_2011-2015_OzFlux_{message}.nc',       \n",
    "                        f'AU-Emr_2012-2013_OzFlux_{message}.nc',       \n",
    "                        f'AU-GWW_2013-2017_OzFlux_{message}.nc',       \n",
    "                        f'AU-Gin_2012-2017_OzFlux_{message}.nc',       \n",
    "                        f'AU-How_2003-2017_OzFlux_{message}.nc',       \n",
    "                        f'AU-Lit_2016-2017_OzFlux_{message}.nc',       \n",
    "                        f'AU-Rig_2011-2016_OzFlux_{message}.nc',       \n",
    "                        f'AU-Rob_2014-2017_OzFlux_{message}.nc',       \n",
    "                        f'AU-Sam_2011-2017_OzFlux_{message}.nc',       \n",
    "                        f'AU-Stp_2010-2017_OzFlux_{message}.nc',       \n",
    "                        f'AU-TTE_2013-2017_OzFlux_{message}.nc',       \n",
    "                        f'AU-Tum_2002-2017_OzFlux_{message}.nc',       \n",
    "                        f'AU-Whr_2015-2016_OzFlux_{message}.nc',       \n",
    "                        f'AU-Wrr_2016-2017_OzFlux_{message}.nc',       \n",
    "                        f'AU-Ync_2011-2017_OzFlux_{message}.nc',\n",
    "                        # f'AU-Otw_2009-2010_OzFlux_{message}.nc',     \n",
    "                        # f'AU-Cow_2010-2015_OzFlux_{message}.nc',         \n",
    "                        # f'AU-Ctr_2010-2017_OzFlux_{message}.nc',         \n",
    "                        ]\n",
    "\n",
    "site_characters    = pd.read_csv('/g/data/w97/mm3972/scripts/PLUMBER2/LSM_GPP_PLUMBER2/txt/site_character.csv')\n",
    "\n",
    "# Correct filtering for Australian sites\n",
    "au_site_characters = site_characters[site_characters['site_name'].str.contains('AU')]\n",
    "au_site_list       = np.unique(au_site_characters['site_name'])\n",
    "\n",
    "for PLUMBER2_AU_file in PLUMBER2_AU_files:\n",
    "\n",
    "    site_name, lat, lon, df_LAI_P2  = get_PLUMBER2_lat_lon_LAI(PLUMBER2_AU_file, au_site_list, start_date, end_date)\n",
    "    \n",
    "    if ~np.isnan(lat):\n",
    "        # Find the indices of the nearest pixels to lat and lon.\n",
    "        lat_idx      = np.argmin(np.abs(lat_lai - lat))\n",
    "        lon_idx      = np.argmin(np.abs(lon_lai - lon))\n",
    "\n",
    "        # Read the climate_class value of the nearest pixel.\n",
    "        df_LAI       = pd.DataFrame(LAI[:, lat_idx, lon_idx].data, columns=['LAI'])\n",
    "\n",
    "        # Read the climate_class value of the nearest pixel.\n",
    "        df_LAI_SG    = pd.DataFrame(LAI_SG[:, lat_idx, lon_idx].data, columns=['LAI'])\n",
    "\n",
    "        # ================== Start Plotting =================\n",
    "        fig, ax  = plt.subplots(figsize=(6,5))\n",
    "\n",
    "        ax.plot(time, df_LAI['LAI'].values, c='red', label='LAI')\n",
    "        ax.plot(time, df_LAI_SG['LAI'].values, c='green', label='LAI_SG')\n",
    "        ax.plot(time, df_LAI_P2['LAI'].values, c='blue', label='LAI_P2')\n",
    "        ax.legend()\n",
    "        ax.set_title(f'LAI Comparison for {site_name}')\n",
    "\n",
    "        plt.savefig(f'./Check_LAI_gridinfo_{site_name}_{year}.png',dpi=300)\n",
    "        \n",
    "        df_albedo            = pd.DataFrame(albedo[0, :, lat_idx, lon_idx].data, columns=['albedo1'])\n",
    "        df_albedo['albedo2'] = albedo[1, :, lat_idx, lon_idx].data\n",
    "        df_albedo['albedo3'] = albedo[2, :,  lat_idx, lon_idx].data\n",
    "        df_albedo['albedo4'] = albedo[3, :,  lat_idx, lon_idx].data\n",
    "        \n",
    "        # Pearson correlation\n",
    "        correlation_LAI_a1, p_value_LAI_a1       = pearsonr(df_LAI['LAI'], df_albedo['albedo1'])\n",
    "        correlation_LAI_a2, p_value_LAI_a2       = pearsonr(df_LAI['LAI'], df_albedo['albedo2'])\n",
    "        correlation_LAI_a3, p_value_LAI_a3       = pearsonr(df_LAI['LAI'], df_albedo['albedo3'])\n",
    "        correlation_LAI_a4, p_value_LAI_a4       = pearsonr(df_LAI['LAI'], df_albedo['albedo4'])\n",
    "        \n",
    "        correlation_LAI_SG_a1, p_value_LAI_SG_a1 = pearsonr(df_LAI_SG['LAI'], df_albedo['albedo1'])\n",
    "        correlation_LAI_SG_a2, p_value_LAI_SG_a2 = pearsonr(df_LAI_SG['LAI'], df_albedo['albedo2'])\n",
    "        correlation_LAI_SG_a3, p_value_LAI_SG_a3 = pearsonr(df_LAI_SG['LAI'], df_albedo['albedo3'])\n",
    "        correlation_LAI_SG_a4, p_value_LAI_SG_a4 = pearsonr(df_LAI_SG['LAI'], df_albedo['albedo4'])\n",
    "        \n",
    "        correlation_LAI_P2_a1, p_value_LAI_P2_a1 = pearsonr(df_LAI_P2['LAI'], df_albedo['albedo1'])\n",
    "        correlation_LAI_P2_a2, p_value_LAI_P2_a2 = pearsonr(df_LAI_P2['LAI'], df_albedo['albedo2'])\n",
    "        correlation_LAI_P2_a3, p_value_LAI_P2_a3 = pearsonr(df_LAI_P2['LAI'], df_albedo['albedo3'])\n",
    "        correlation_LAI_P2_a4, p_value_LAI_P2_a4 = pearsonr(df_LAI_P2['LAI'], df_albedo['albedo4'])\n",
    "        print(f'======= {site_name} =======')\n",
    "        print(correlation_LAI_a1, correlation_LAI_a2, correlation_LAI_a3, correlation_LAI_a4)\n",
    "        print(correlation_LAI_SG_a1, correlation_LAI_SG_a2, correlation_LAI_SG_a3, correlation_LAI_SG_a4)\n",
    "        print(correlation_LAI_P2_a1, correlation_LAI_P2_a2, correlation_LAI_P2_a3, correlation_LAI_P2_a4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a01ebbf-2f80-4887-92d5-cdd4808cc4f3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Calculate correl spatial map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f399420b-61e6-4285-9717-2fb6c68729b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute correlation map\n",
    "def compute_correlation_map(data1, data2):\n",
    "    \"\"\"\n",
    "    Calculate the correlation map between two 3D arrays over time.\n",
    "    \n",
    "    Args:\n",
    "        data1: 3D array of shape (time, lat, lon)\n",
    "        data2: 3D array of shape (time, lat, lon)\n",
    "    \n",
    "    Returns:\n",
    "        2D correlation map (lat, lon)\n",
    "    \"\"\"\n",
    "    assert data1.shape == data2.shape, \"Arrays must have the same shape\"\n",
    "    \n",
    "    correlation_map = np.full((data1.shape[1], data1.shape[2]), np.nan)\n",
    "    \n",
    "    for i in range(data1.shape[1]):  # Iterate over latitudes\n",
    "        for j in range(data1.shape[2]):  # Iterate over longitudes\n",
    "            series1 = data1[:, i, j]\n",
    "            series2 = data2[:, i, j]\n",
    "            \n",
    "            # Skip grid points with NaN values\n",
    "            if np.any(np.isnan(series1)) or np.any(np.isnan(series2)):\n",
    "                correlation_map[i, j] = np.nan\n",
    "                continue\n",
    "            \n",
    "            correlation_map[i, j] = np.corrcoef(series1, series2)[0, 1]\n",
    "    \n",
    "    return correlation_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c2c0ce-283b-4e8b-8fd8-4aa57a662ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as nc\n",
    "from scipy.stats import pearsonr\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Input year\n",
    "# year = 2017\n",
    "for albedo_num in np.arange(1,4):\n",
    "    for year in np.arange(2000,2021):\n",
    "        # File paths\n",
    "        file_gridinfo_SG = f'/g/data/w97/mm3972/scripts/Land_Drought_Rainfall/process_for_CABLE/nc_files/gridinfo_AWAP_OpenLandMap_ELEV_DLCM_fix_MODIS_LAI_albedo_lc_time_varying_{year}.nc'\n",
    "        file_gridinfo    = f'/g/data/w97/mm3972/scripts/Land_Drought_Rainfall/process_for_CABLE/nc_files/no_SG_31-day_running_smooth/gridinfo_AWAP_OpenLandMap_ELEV_DLCM_fix_MODIS_LAI_albedo_lc_time_varying_{year}.nc'\n",
    "\n",
    "        # Load data from NetCDF files\n",
    "        f_SG    = nc.Dataset(file_gridinfo_SG)\n",
    "        LAI_SG  = f_SG.variables['LAI'][:]  # [time, lat, lon]\n",
    "        albedo  = f_SG.variables['Albedo_MODIS'][:]  # [time, lat, lon]\n",
    "        lat_lai = f_SG.variables['latitude'][:]\n",
    "        lon_lai = f_SG.variables['longitude'][:]\n",
    "\n",
    "        f       = nc.Dataset(file_gridinfo)\n",
    "        LAI     = f.variables['LAI'][:]  # [time, lat, lon]\n",
    "\n",
    "        # Generate time array for processed LAI\n",
    "        start_date = datetime(year, 1, 1, 0, 0)\n",
    "        end_date   = datetime(year, 12, 31, 0, 0)\n",
    "        time       = np.array([start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)])\n",
    "\n",
    "        # Compute the correlation map\n",
    "        correlation_map_SG = compute_correlation_map(LAI_SG, albedo[albedo_num,:,:,:])\n",
    "        correlation_map    = compute_correlation_map(LAI, albedo[albedo_num,:,:,:])\n",
    "\n",
    "        # ================== Start Plotting =================\n",
    "        fig, axs  = plt.subplots(nrows=1, ncols=3, figsize=(6,5), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "        levels1   = [-0.95, -0.9,-0.85,-0.8,-0.75,-0.7,-0.65,-0.6,-0.55,-0.5,-0.45,-0.4,-0.35,-0.3,-0.25,-0.2, -0.15,-0.1,-0.05, 0,\n",
    "                      0.05,  0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8,  0.85, 0.9, 0.95]\n",
    "        levels2   = [-0.5,-0.45,-0.4,-0.35,-0.3,-0.25,-0.2, -0.15,-0.1,-0.05, 0.05,  0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, ]\n",
    "\n",
    "        plot1     = axs[0].contourf(lon_lai, lat_lai, correlation_map_SG, levels=levels1, cmap=\"coolwarm\", extend=\"both\")\n",
    "        cb1       = plt.colorbar(plot1, ax=axs[0], label=\"LAI_SG vs albedo\", orientation=\"horizontal\", pad=0.02, aspect=16, shrink=0.8)\n",
    "        cb1.ax.tick_params(labelsize=9)\n",
    "\n",
    "        plot2     = axs[1].contourf(lon_lai, lat_lai, correlation_map, levels=levels1, cmap=\"coolwarm\", extend=\"both\")\n",
    "        cb2       = plt.colorbar(plot2, ax=axs[1],label=\"LAI vs albedo\",orientation=\"horizontal\", pad=0.02, aspect=16, shrink=0.8)\n",
    "        cb2.ax.tick_params(labelsize=9)\n",
    "\n",
    "        plot3     = axs[2].contourf(lon_lai, lat_lai, np.abs(correlation_map_SG)-np.abs(correlation_map), levels=20, cmap=\"coolwarm\", extend=\"both\")\n",
    "        cb3       = plt.colorbar(plot3, ax=axs[2],label=\"abs(SG) - abs(noSG)\",orientation=\"horizontal\", pad=0.02, aspect=16, shrink=0.8)\n",
    "        cb3.ax.tick_params(labelsize=9)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'/g/data/w97/mm3972/scripts/Land_Drought_Rainfall/plots/Check_LAI/Check_LAI_albedo_correl_{year}_albedo={albedo_num+1}.png',dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891baacf-d026-4810-b49f-c55bf2990fab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis3-24.07]",
   "language": "python",
   "name": "conda-env-analysis3-24.07-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
